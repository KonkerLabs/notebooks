{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atividade Experimental - ImageDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4Bzafqm7lrd"
      },
      "source": [
        "# Guia para a aula experimental\n",
        "Neste guia trataremos do uso de um sensor virtual, que utiliza fotografias para o cálculo de pessoas em uma localidade. Nesse guia vocês poderão testar diversos modelos de Deep Learning, indo desde uma Efficient Net até uma Mask-RCNN em diversas fotografias diferentes: dos restaurantes da Unicamp (em tempo real) e alguns exemplos encontrados na Web de Toronto, Boston e Nova York.No final você poderá enviar o resultado da inferência para a plataforma Konker, completando o ciclo completo de análise na nuvem e envio de dados processados para uma plataforma de IoT.Para seguir a parte final do roteiro, você precisará de uma conta na Konker, que pode ser criada em https://demo.konkerlabs.net/.\n",
        "\n",
        "Para rodar esse Guia, você pode executar as células (exceto as que contêm listas de seleção) até chegar na célula contendo as usuário e senha de acesso ao dispostivo na plataforma Konker. Essa é a única célula na qual é necessária modificação, no caso, com a escrita das credenciais.\n",
        "\n",
        "Esse roteiro utiliza material do trabalho publicado pelo Google, que pode ser encontrado em: https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bOx-b2-fM7"
      },
      "source": [
        "###Vamos garantir primeiro que a versão instalada do Tensorflow é igual ou superior a 2.6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znMEiYU6KIfe"
      },
      "source": [
        "%%capture\n",
        "!pip install -U \"tensorflow>=2.5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPqJs6VS-xpj"
      },
      "source": [
        "###Carregando as bibliotecas que serão usadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z8LbxdxQfLQ"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy2m7h3o-5Fz"
      },
      "source": [
        "###Definindo a função de leitura das imagens, os modelos e as imagens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFoa77ztKduX"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "ALL_MODELS = {\n",
        "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
        "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
        "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
        "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "}\n",
        "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10), (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
        "IMAGES_FOR_TEST = {\n",
        "  'RU - Unicamp 1' : 'https://webservices.prefeitura.unicamp.br/cameras/cam_ru_a.jpg',\n",
        "  'RU - Unicamp 2' : 'https://webservices.prefeitura.unicamp.br/cameras/cam_ru_b.jpg',\n",
        "  'RA - Unicamp' : 'https://webservices.prefeitura.unicamp.br/cameras/cam_ra.jpg',\n",
        "  'RS - Unicamp' : 'https://webservices.prefeitura.unicamp.br/cameras/cam_rs.jpg',\n",
        "  'Toronto' : 'https://images.thestar.com/Sic8O1_yOZdZ_htIsDM7dfq33t0=/1280x1024/smart/filters:cb(1516743858004)/https://www.thestar.com/content/dam/thestar/life/food_wine/2012/10/04/toronto_restaurant_lineups_just_part_of_todays_eating_experience/grandelectric.jpeg',\n",
        "  'dreamstime.com' : 'https://thumbs.dreamstime.com/z/nyc-people-waiting-outside-restaurant-table-20995159.jpg',\n",
        "  'BostonGlobe' : 'https://bostonglobe-prod.cdn.arcpublishing.com/resizer/kOd-PMQUsCOkoEqK861NUX1XDQ4=/1440x0/arc-anglerfish-arc2-prod-bostonglobe.s3.amazonaws.com/public/2P7FQXFU3EI6HLDEZS3P4FVF7I.jpg',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QjarYVQ_Nd2"
      },
      "source": [
        "###Clonando o repositório de modelos do tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX9Ycf5TMQKD"
      },
      "source": [
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw5WkDyO_T7K"
      },
      "source": [
        "###Instalando a Image API do Tensorflow e compilando as bibliotecas Protobuf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-BUoc3lMWmO"
      },
      "source": [
        "%%capture\n",
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilw5SArN_4jU"
      },
      "source": [
        "###Importando as bibliotecas de visualização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "492354DcMjCW"
      },
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9XmXOWo_8hh"
      },
      "source": [
        "###Importando as legendas do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kCpv5_mNcgH"
      },
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTL2drzgAATq"
      },
      "source": [
        "###Na célula abaixo, você poderá selecionar o modelo de Deep Learning que será utilizado. Não é necessário rodar essa célula, basta apenas selecionar o modelo na lista. \n",
        "###Você pode voltar nessa célula em qualquer momento para testar o desempenho de outro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4L_rkbEOqbO"
      },
      "source": [
        "#Agora vamos selecionar o modelo a ser usado\n",
        "#@title Seleção do modelo { display-mode: \"form\", run: \"auto\" }\n",
        "model_display_name = 'EfficientDet D0 512x512' # @param ['EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Modelo Selecionado:'+ model_display_name)\n",
        "print('Link do modelo no TensorFlow Hub: {}'.format(model_handle))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSYjDYiTOANd"
      },
      "source": [
        "#Lendo o modelo selecionado\n",
        "print('Lendo modelo...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('Modelo carregado!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2pTjbvGAwyL"
      },
      "source": [
        "###Da mesma forma que na seleção do modelo, não é necessário rodar a célula abaixo, é necessário apenas selecionar a imagem na lista. As primeiras imagens da lista são obtidas em tempo real das câmeras dos restaurantes da Unicamp. Elas estão disponíveis publicamente na Web, nos endereços exibidos nesse notebook.\n",
        "###As 3 últimas fotografias são imagens obtidas na Web das cidades de Toronto, Boston e Nova York. Elas também estão disponíveis na Web, nos endereços disponíveis nesse notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDeqeNxHOWHo"
      },
      "source": [
        "#@title Seleção da Imagem { display-mode: \"form\", run: \"auto\" }\n",
        "selected_image = 'BostonGlobe' # @param ['RU - Unicamp 1', 'RU - Unicamp 2', 'RA - Unicamp','RS - Unicamp', 'Toronto', 'dreamstime.com', 'BostonGlobe']\n",
        "flip_image_horizontally = False #@param {type:\"boolean\"}\n",
        "convert_image_to_grayscale = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBXx0YdICgYZ"
      },
      "source": [
        "###Obtendo a imagem da Web e exibindo no notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLTSABSGQ7k4"
      },
      "source": [
        "image_path = IMAGES_FOR_TEST[selected_image]\n",
        "print('Baixando: '+ selected_image + ' , em: '+ image_path)\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "\n",
        "# Flip horizontally\n",
        "if(flip_image_horizontally):\n",
        "  image_np[0] = np.fliplr(image_np[0]).copy()\n",
        "\n",
        "# Convert image to grayscale\n",
        "if(convert_image_to_grayscale):\n",
        "  image_np[0] = np.tile(\n",
        "    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZBWIi-wCpf7"
      },
      "source": [
        "###Rodando a inferência do modelo escolhido"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEJfmHGlQ8bF"
      },
      "source": [
        "results = hub_model(image_np)\n",
        "result = {key:value.numpy() for key,value in results.items()}\n",
        "#print(result.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWdqrnkECw1t"
      },
      "source": [
        "###Visualizando os resultados:\n",
        "Obs: Você pode mudar o threshold de detecção alterando o valor da variável de mesmo nome abaixo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T4teJ0sTKzY"
      },
      "source": [
        "label_id_offset = 0\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "#Você pode mudar o threshold de detecção mudando o parâmetro abaixo:\n",
        "threshold = 0.5\n",
        "\n",
        "keypoints, keypoint_scores = None, None\n",
        "if 'detection_keypoints' in result:\n",
        "  keypoints = result['detection_keypoints'][0]\n",
        "  keypoint_scores = result['detection_keypoint_scores'][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections[0],\n",
        "      result['detection_boxes'][0],\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=threshold,\n",
        "      agnostic_mode=False,\n",
        "      keypoints=keypoints,\n",
        "      keypoint_scores=keypoint_scores,\n",
        "      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
        "\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uMnMm_RDDLj"
      },
      "source": [
        "###Contando agora o número de detecções de pessoas na imagem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqSwNbyTRGc"
      },
      "source": [
        "person_count = 0\n",
        "for i,j in enumerate(results['detection_scores'][0]):\n",
        "  if j > threshold:\n",
        "    if results['detection_classes'][0][i] == 1:\n",
        "      person_count = person_count+1\n",
        "\n",
        "print('Pessoas detectadas: ' + str(person_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my_GM2MFD1UI"
      },
      "source": [
        "###Vamos agora fazer a primeira modificação em uma célula.\n",
        "###Crie um dispositivo na plataforma Konker e escreva as credenciais do **dispositivo** na célula abaixo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptQyNlddwh2D"
      },
      "source": [
        "sensor_user = ''\n",
        "sensor_pass = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc9uiA7zEI51"
      },
      "source": [
        "###Definindo agora as funções que usaremos para enviar os dados via REST para a plataforma. Nessa função o nome do canal foi configurado como **person_detector**, mas isso pode ser modificado na variável **canal**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVTIkXgeUM8R"
      },
      "source": [
        "current_milli_time = lambda: int(round(time.time() * 1000))\n",
        "\n",
        "def post_data(user, passwd):\n",
        "  global model_display_name, selected_image, person_count\n",
        "  canal = 'person_detector'\n",
        "  ts = current_milli_time()\n",
        "  data = json.dumps({'_ts': ts, 'model': model_display_name, 'image' : selected_image, 'person_count':person_count})\n",
        "  req = requests.post('http://data.demo.konkerlabs.net:80/pub/'+user+'/'+ canal, auth=(user, passwd), data=data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF0HsW8KE-kL"
      },
      "source": [
        "###Por último, vamos enviar os dados para a plataforma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZSV0gj-wnrJ"
      },
      "source": [
        "post_data(sensor_user, sensor_pass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QWvBmzoFHAT"
      },
      "source": [
        "###Caso tudo tenha funcionado corretamente, você deve ver uma mensagem chegando no seu dispositivo da plataforma contendo os dados do nome da imagem, do modelo de Deep Learning, timestamp e contagem de pessoas. Agora você pode retornar nas células anteriores e modificar o modelo e/ou modificar a fotografia onde ele é aplicado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UUJs6zSzTIW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}